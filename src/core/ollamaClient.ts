import inquirer from 'inquirer';
import axios from 'axios';
import chalk from 'chalk';
import { loadConfig, saveConfig } from '../utils/configHelper.js';

const OLLAMA_URL = 'http://localhost:11434';

export async function chatWithOllama() {
  console.log(chalk.yellow('\nüó£Ô∏è Chat with Ollama\n'));

  let modelList: string[] = [];

  try {
    const res = await axios.get(`${OLLAMA_URL}/api/tags`);
    modelList = res.data.models.map((m: any) => m.name);
  } catch (error: any) {
    console.error(chalk.red(`‚ùå Failed to fetch models: ${error.message}`));
    return;
  }

  if (modelList.length === 0) {
    console.error(chalk.red('‚ùå No models found. Please run: ollama pull <model-name>'));
    return;
  }

  const config = loadConfig();
  const defaultModel = config.chatModel && modelList.includes(config.chatModel)
    ? config.chatModel
    : modelList[0];

  console.log('\nAvailable models:');
  modelList.forEach((model, index) => {
    const isDefault = model === defaultModel ? ' (default)' : '';
    console.log(`${index + 1}. ${model}${isDefault}`);
  });

  const { modelIndex } = await inquirer.prompt([
    {
      type: 'input',
      name: 'modelIndex',
      message: `Enter model number to use (1‚Äì${modelList.length}):`,
      validate: (input) => {
        const n = Number(input);
        return n >= 1 && n <= modelList.length || `Please enter a number between 1 and ${modelList.length}`;
      }
    }
  ]);

  const selectedModel = modelList[Number(modelIndex) - 1];
  saveConfig({ chatModel: selectedModel });

  console.log(chalk.gray(`\nüß† Using model: ${selectedModel}`));
  console.log(chalk.gray(`Type ':menu' or 'exit' anytime to return to main menu.\n`));

  while (true) {
    const { userInput } = await inquirer.prompt([
      {
        type: 'input',
        name: 'userInput',
        message: chalk.cyan('You:')
      }
    ]);

    const input = userInput.trim().toLowerCase();

    if (['exit', 'quit', ':exit', ':menu', ':back'].includes(input)) {
      console.log(chalk.green('\nüëã Exiting chat. Returning to main menu...\n'));
      return; // ‚úÖ Return to main menu
    }

    try {
      const res = await axios.post(`${OLLAMA_URL}/api/chat`, {
        model: selectedModel,
        messages: [{ role: 'user', content: userInput }],
        stream: false
      });

      const reply = res.data.message?.content || '[No response from model]';
      console.log(chalk.magentaBright(`\nüß† Ollama: ${reply}\n`));
    } catch (error: any) {
      console.error(chalk.red(`‚ùå Error from Ollama: ${error.message}`));
    }
  }
}
